{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04: Model Training - MobileNetV2 Skin Lesion Classifier\n",
    "\n",
    "## BACHELOR'S CAPSTONE PROJECT: DermoAI\n",
    "\n",
    "**Objective:** Train a production-quality MobileNetV2 CNN for FST V-VI skin lesion classification.\n",
    "\n",
    "### Research Proposal Goals\n",
    "- **Objective 2:** Develop AI triage system with ≥80% recall on urgent cases for FST V-VI\n",
    "- **Objective 4:** Evaluate clinical utility with FST-stratified metrics\n",
    "\n",
    "### Success Criteria\n",
    "- Recall ≥80% on urgent cases (malignant conditions)\n",
    "- FST V vs VI performance gap < 10%\n",
    "- Per-class recall > 50% for all 12 classes\n",
    "- Zero or minimal high-risk errors (malignant to benign)\n",
    "- Reproducible results with clear documentation\n",
    "\n",
    "### Dataset Summary\n",
    "- **Total Images:** 2,155 (FST V-VI only)\n",
    "- **Training:** 2,939 (augmented)\n",
    "- **Validation:** 324 (original)\n",
    "- **Test:** 324 (original)\n",
    "- **Classes:** 12 (8 independent + 4 grouped)\n",
    "\n",
    "### Critical Challenges\n",
    "- **Class imbalance:** inflammatory class (34% of training data) will dominate without class weighting\n",
    "- **FST equity:** Must maintain <10% performance gap between FST V and VI\n",
    "- **Small test set:** Results may have higher variance (n=324)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core ML libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import (\n",
    "    ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    ")\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import json\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix,\n",
    "    recall_score, precision_score, f1_score\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "print(f\"Random seed set to: {RANDOM_SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(f\"GPU Available: {len(gpus) > 0}\")\n",
    "\n",
    "if gpus:\n",
    "    print(f\"GPU detected: {gpus[0].name}\")\n",
    "    print(\"  Training will be fast.\")\n",
    "    \n",
    "    # Enable memory growth to avoid OOM\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"  Memory growth enabled\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"  Memory growth error: {e}\")\n",
    "else:\n",
    "    print(\"No GPU detected; training will be slow\")\n",
    "    print(\"Consider using Google Colab with GPU runtime\")\n",
    "\n",
    "print(f\"\\nTensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration & Paths\n",
    "\n",
    "Supports both local and Google Colab execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment configuration\n",
    "USE_GOOGLE_DRIVE = False  # Set True for Google Colab\n",
    "\n",
    "# Mount Google Drive if needed\n",
    "if USE_GOOGLE_DRIVE:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    BASE_DIR = Path('/content/drive/MyDrive/dermoai')\n",
    "    print(\"Google Drive mounted.\")\n",
    "else:\n",
    "    BASE_DIR = Path('../')  # Local execution\n",
    "    print(\"Using local filesystem.\")\n",
    "\n",
    "# Data directories\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "TRAIN_DIR = DATA_DIR / 'augmented' / 'train'  # Use augmented training data\n",
    "VAL_DIR = DATA_DIR / 'processed' / 'fitzpatrick17k' / 'val'\n",
    "TEST_DIR = DATA_DIR / 'processed' / 'fitzpatrick17k' / 'test'\n",
    "\n",
    "# Output directories\n",
    "RESULTS_DIR = BASE_DIR / 'results' / 'training'\n",
    "MODEL_DIR = BASE_DIR / 'models'\n",
    "\n",
    "# Create directories\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"\\nPath Configuration:\")\n",
    "print(f\"  Base directory: {BASE_DIR}\")\n",
    "print(f\"  Training data: {TRAIN_DIR}\")\n",
    "print(f\"  Validation data: {VAL_DIR}\")\n",
    "print(f\"  Test data: {TEST_DIR}\")\n",
    "print(f\"  Results output: {RESULTS_DIR}\")\n",
    "print(f\"  Model output: {MODEL_DIR}\")\n",
    "\n",
    "# Verify directories exist\n",
    "for dir_path in [TRAIN_DIR, VAL_DIR, TEST_DIR]:\n",
    "    if not dir_path.exists():\n",
    "        raise FileNotFoundError(f\"Directory not found: {dir_path}\")\n",
    "\n",
    "print(\"\\nAll directories verified.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "CONFIG = {\n",
    "    'model': {\n",
    "        'architecture': 'MobileNetV2',\n",
    "        'input_shape': (224, 224, 3),\n",
    "        'num_classes': 12,\n",
    "        'weights': 'imagenet',\n",
    "        'dropout_rate': 0.5\n",
    "    },\n",
    "    'training': {\n",
    "        'batch_size': 32,\n",
    "        'phase1_epochs': 30,  # Train classifier head\n",
    "        'phase2_epochs': 20,  # Fine-tune top layers\n",
    "        'initial_lr': 1e-4,\n",
    "        'finetune_lr': 1e-5,\n",
    "        'use_class_weights': True\n",
    "    },\n",
    "    'callbacks': {\n",
    "        'early_stopping_patience': 15,\n",
    "        'reduce_lr_patience': 5,\n",
    "        'reduce_lr_factor': 0.5,\n",
    "        'min_lr': 1e-7\n",
    "    },\n",
    "    'evaluation': {\n",
    "        'urgent_classes': ['squamous_cell_carcinoma', 'malignant_skin_lesions'],\n",
    "        'benign_classes': ['benign_neoplastic_lesions'],\n",
    "        'min_recall_threshold': 0.50,\n",
    "        'urgent_recall_target': 0.80,\n",
    "        'fst_equity_gap_threshold': 0.10\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(json.dumps(CONFIG, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading & Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count images in each directory\n",
    "def count_images_per_class(data_dir):\n",
    "    \"\"\"Count images per class in a directory.\"\"\"\n",
    "    class_counts = {}\n",
    "    for class_dir in sorted(data_dir.iterdir()):\n",
    "        if class_dir.is_dir():\n",
    "            image_files = list(class_dir.glob('*.jpg'))\n",
    "            class_counts[class_dir.name] = len(image_files)\n",
    "    return class_counts\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA LOADING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Count images\n",
    "train_counts = count_images_per_class(TRAIN_DIR)\n",
    "val_counts = count_images_per_class(VAL_DIR)\n",
    "test_counts = count_images_per_class(TEST_DIR)\n",
    "\n",
    "train_total = sum(train_counts.values())\n",
    "val_total = sum(val_counts.values())\n",
    "test_total = sum(test_counts.values())\n",
    "\n",
    "print(f\"\\nTraining: {train_total} images (augmented)\")\n",
    "print(f\"Validation: {val_total} images (original)\")\n",
    "print(f\"Test: {test_total} images (original)\")\n",
    "print(f\"\\nTotal classes: {len(train_counts)}\")\n",
    "\n",
    "# Display class distribution\n",
    "print(\"\\nClass Distribution (Training):\")\n",
    "sorted_classes = sorted(train_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "for class_name, count in sorted_classes:\n",
    "    pct = count / train_total * 100\n",
    "    status = \"(dominant)\" if pct > 30 else \"(smallest)\" if count < 120 else \"\"\n",
    "    print(f\"  {class_name:35s}: {count:4d} ({pct:5.1f}%) {status}\")\n",
    "\n",
    "# Check for severe imbalance\n",
    "max_count = max(train_counts.values())\n",
    "min_count = min(train_counts.values())\n",
    "imbalance_ratio = max_count / min_count\n",
    "\n",
    "print(f\"\\nClass Imbalance Ratio: {imbalance_ratio:.1f}:1\")\n",
    "if imbalance_ratio > 5:\n",
    "    print(\"WARNING: Severe class imbalance detected\")\n",
    "    print(\"Class weighting is critical for this dataset\")\n",
    "else:\n",
    "    print(\"Class imbalance is manageable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing & Generators\n",
    "\n",
    "Create TensorFlow data generators with appropriate normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data generator (augmentation already applied to files)\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,  # Normalize to [0,1]\n",
    "    # Note: Spatial augmentation already done during preprocessing\n",
    ")\n",
    "\n",
    "# Validation/Test generators (no augmentation)\n",
    "val_test_datagen = ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "# Create generators\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=CONFIG['model']['input_shape'][:2],\n",
    "    batch_size=CONFIG['training']['batch_size'],\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    seed=RANDOM_SEED\n",
    ")\n",
    "\n",
    "val_generator = val_test_datagen.flow_from_directory(\n",
    "    VAL_DIR,\n",
    "    target_size=CONFIG['model']['input_shape'][:2],\n",
    "    batch_size=CONFIG['training']['batch_size'],\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_generator = val_test_datagen.flow_from_directory(\n",
    "    TEST_DIR,\n",
    "    target_size=CONFIG['model']['input_shape'][:2],\n",
    "    batch_size=CONFIG['training']['batch_size'],\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(\"\\nData generators created successfully.\")\n",
    "print(f\"\\nClass indices mapping:\")\n",
    "class_indices = train_generator.class_indices\n",
    "class_names = list(class_indices.keys())\n",
    "for class_name, idx in sorted(class_indices.items(), key=lambda x: x[1]):\n",
    "    print(f\"  {idx:2d}: {class_name}\")\n",
    "\n",
    "# Save class names\n",
    "with open(MODEL_DIR / 'class_names.json', 'w') as f:\n",
    "    json.dump(class_names, f, indent=2)\n",
    "print(f\"\\nClass names saved to {MODEL_DIR / 'class_names.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compute Class Weights\n",
    "\n",
    "**CRITICAL:** Class weighting prevents the model from being biased toward the dominant \"inflammatory\" class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG['training']['use_class_weights']:\n",
    "    # Get class labels from training data\n",
    "    train_labels = train_generator.classes\n",
    "    \n",
    "    # Compute balanced class weights\n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=np.unique(train_labels),\n",
    "        y=train_labels\n",
    "    )\n",
    "    \n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"CLASS WEIGHTS (higher weight = more important)\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\n{'Class':<35} {'Samples':<10} {'Weight':<10}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for idx, class_name in enumerate(class_names):\n",
    "        weight = class_weight_dict[idx]\n",
    "        count = np.sum(train_labels == idx)\n",
    "        print(f\"{class_name:<35} {count:<10} {weight:<10.2f}\")\n",
    "    \n",
    "    print(\"\\nClass weights applied to reduce inflammatory dominance.\")\n",
    "    print(\"  Smaller classes get higher weights during training\")\n",
    "    \n",
    "else:\n",
    "    class_weight_dict = None\n",
    "    print(\"Class weighting is disabled; model may be biased.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Architecture\n",
    "\n",
    "MobileNetV2 with custom classification head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_classes=12, input_shape=(224, 224, 3), dropout_rate=0.5):\n",
    "    \"\"\"\n",
    "    Create MobileNetV2-based model for skin lesion classification.\n",
    "    \n",
    "    Architecture:\n",
    "    - MobileNetV2 backbone (pretrained on ImageNet)\n",
    "    - GlobalAveragePooling2D\n",
    "    - Dense(256, relu) + Dropout(0.5)\n",
    "    - Dense(num_classes, softmax)\n",
    "    \n",
    "    Args:\n",
    "        num_classes: Number of output classes\n",
    "        input_shape: Input image shape (H, W, C)\n",
    "        dropout_rate: Dropout rate for regularization\n",
    "        \n",
    "    Returns:\n",
    "        model: Keras Model\n",
    "        base_model: MobileNetV2 base model (for fine-tuning later)\n",
    "    \"\"\"\n",
    "    # Load pretrained MobileNetV2\n",
    "    base_model = MobileNetV2(\n",
    "        input_shape=input_shape,\n",
    "        include_top=False,\n",
    "        weights='imagenet'\n",
    "    )\n",
    "    \n",
    "    # Freeze base model initially (will unfreeze later for fine-tuning)\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    # Build classification head\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = keras.Model(inputs, outputs, name='MobileNetV2_Dermatology')\n",
    "    \n",
    "    return model, base_model\n",
    "\n",
    "# Create model\n",
    "print(\"Creating model...\")\n",
    "model, base_model = create_model(\n",
    "    num_classes=CONFIG['model']['num_classes'],\n",
    "    input_shape=CONFIG['model']['input_shape'],\n",
    "    dropout_rate=CONFIG['model']['dropout_rate']\n",
    ")\n",
    "\n",
    "print(\"\\nModel created successfully.\\n\")\n",
    "model.summary()\n",
    "\n",
    "# Count parameters\n",
    "total_params = model.count_params()\n",
    "trainable_params = sum([tf.size(w).numpy() for w in model.trainable_weights])\n",
    "non_trainable_params = total_params - trainable_params\n",
    "\n",
    "print(f\"\\nModel Parameters:\")\n",
    "print(f\"  Total: {total_params:,}\")\n",
    "print(f\"  Trainable: {trainable_params:,}\")\n",
    "print(f\"  Non-trainable: {non_trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "callbacks = [\n",
    "    # Save best model based on validation loss\n",
    "    ModelCheckpoint(\n",
    "        MODEL_DIR / 'best_model.keras',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        mode='min',\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Early stopping to prevent overfitting\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=CONFIG['callbacks']['early_stopping_patience'],\n",
    "        restore_best_weights=True,\n",
    "        mode='min',\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Reduce learning rate on plateau\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=CONFIG['callbacks']['reduce_lr_factor'],\n",
    "        patience=CONFIG['callbacks']['reduce_lr_patience'],\n",
    "        min_lr=CONFIG['callbacks']['min_lr'],\n",
    "        mode='min',\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Log training history\n",
    "    CSVLogger(\n",
    "        RESULTS_DIR / 'training_history.csv',\n",
    "        append=False\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"Callbacks configured:\")\n",
    "print(\"  - ModelCheckpoint (save best model)\")\n",
    "print(f\"  - EarlyStopping (patience={CONFIG['callbacks']['early_stopping_patience']})\")\n",
    "print(f\"  - ReduceLROnPlateau (patience={CONFIG['callbacks']['reduce_lr_patience']})\")\n",
    "print(\"  - CSVLogger (training history)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Phase 1: Train Classifier Head\n",
    "\n",
    "First, train only the classification head with the base model frozen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model for phase 1\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=CONFIG['training']['initial_lr']),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tf.keras.metrics.Recall(name='recall'),\n",
    "        tf.keras.metrics.AUC(name='auc')\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PHASE 1: Training Classifier Head (Base Model Frozen)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Learning rate: {CONFIG['training']['initial_lr']}\")\n",
    "print(f\"Epochs: {CONFIG['training']['phase1_epochs']}\")\n",
    "print(f\"Batch size: {CONFIG['training']['batch_size']}\")\n",
    "print(f\"Class weighting: {'Enabled' if class_weight_dict else 'Disabled'}\")\n",
    "print(\"\\nStarting training...\\n\")\n",
    "\n",
    "# Train phase 1\n",
    "history_phase1 = model.fit(\n",
    "    train_generator,\n",
    "    epochs=CONFIG['training']['phase1_epochs'],\n",
    "    validation_data=val_generator,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nPhase 1 training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training Phase 2: Fine-tune Top Layers\n",
    "\n",
    "Unfreeze top layers of base model and fine-tune with lower learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PHASE 2: Fine-tuning Top Layers\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Unfreeze base model\n",
    "base_model.trainable = True\n",
    "\n",
    "# Freeze all layers except last 20\n",
    "for layer in base_model.layers[:-20]:\n",
    "    layer.trainable = False\n",
    "\n",
    "trainable_layers = sum([layer.trainable for layer in model.layers])\n",
    "print(f\"\\nTrainable layers: {trainable_layers}\")\n",
    "\n",
    "# Re-compile with lower learning rate\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=CONFIG['training']['finetune_lr']),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tf.keras.metrics.Recall(name='recall'),\n",
    "        tf.keras.metrics.AUC(name='auc')\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Learning rate: {CONFIG['training']['finetune_lr']} (10x lower)\")\n",
    "print(f\"Epochs: {CONFIG['training']['phase2_epochs']}\")\n",
    "print(\"\\nStarting fine-tuning...\\n\")\n",
    "\n",
    "# Continue training (phase 2)\n",
    "history_phase2 = model.fit(\n",
    "    train_generator,\n",
    "    epochs=CONFIG['training']['phase2_epochs'],\n",
    "    validation_data=val_generator,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nPhase 2 fine-tuning complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Training History Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine histories from both phases\n",
    "history = {}\n",
    "for key in history_phase1.history.keys():\n",
    "    history[key] = history_phase1.history[key] + history_phase2.history[key]\n",
    "\n",
    "# Plot training history\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Loss\n",
    "axes[0, 0].plot(history['loss'], label='Train Loss', linewidth=2)\n",
    "axes[0, 0].plot(history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[0, 0].axvline(x=len(history_phase1.history['loss']), color='red', linestyle='--', label='Phase 2 Start')\n",
    "axes[0, 0].set_title('Model Loss', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[0, 1].plot(history['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "axes[0, 1].plot(history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
    "axes[0, 1].axvline(x=len(history_phase1.history['loss']), color='red', linestyle='--', label='Phase 2 Start')\n",
    "axes[0, 1].set_title('Model Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Recall\n",
    "axes[1, 0].plot(history['recall'], label='Train Recall', linewidth=2)\n",
    "axes[1, 0].plot(history['val_recall'], label='Val Recall', linewidth=2)\n",
    "axes[1, 0].axvline(x=len(history_phase1.history['loss']), color='red', linestyle='--', label='Phase 2 Start')\n",
    "axes[1, 0].set_title('Model Recall', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Recall')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# AUC\n",
    "axes[1, 1].plot(history['auc'], label='Train AUC', linewidth=2)\n",
    "axes[1, 1].plot(history['val_auc'], label='Val AUC', linewidth=2)\n",
    "axes[1, 1].axvline(x=len(history_phase1.history['loss']), color='red', linestyle='--', label='Phase 2 Start')\n",
    "axes[1, 1].set_title('Model AUC', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('AUC')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Training history saved to {RESULTS_DIR / 'training_history.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Model Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"EVALUATING MODEL ON TEST SET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load best model\n",
    "print(\"\\nLoading best model from checkpoint...\")\n",
    "model = keras.models.load_model(MODEL_DIR / 'best_model.keras')\n",
    "print(\"Best model loaded.\")\n",
    "\n",
    "# Predict on test set\n",
    "print(\"\\nGenerating predictions on test set...\")\n",
    "test_predictions = model.predict(test_generator, verbose=1)\n",
    "test_pred_classes = np.argmax(test_predictions, axis=1)\n",
    "test_true_classes = test_generator.classes\n",
    "\n",
    "print(\"\\nPredictions complete.\")\n",
    "print(f\"  Test samples: {len(test_true_classes)}\")\n",
    "print(f\"  Predictions shape: {test_predictions.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Overall Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"OVERALL TEST PERFORMANCE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Overall metrics\n",
    "test_accuracy = np.mean(test_pred_classes == test_true_classes)\n",
    "test_recall = recall_score(test_true_classes, test_pred_classes, average='macro')\n",
    "test_precision = precision_score(test_true_classes, test_pred_classes, average='macro')\n",
    "test_f1 = f1_score(test_true_classes, test_pred_classes, average='macro')\n",
    "\n",
    "print(f\"\\nTest Accuracy:  {test_accuracy:.1%}\")\n",
    "print(f\"Test Recall:    {test_recall:.1%} (macro)\")\n",
    "print(f\"Test Precision: {test_precision:.1%} (macro)\")\n",
    "print(f\"Test F1-Score:  {test_f1:.1%} (macro)\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PER-CLASS CLASSIFICATION REPORT\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(classification_report(\n",
    "    test_true_classes,\n",
    "    test_pred_classes,\n",
    "    target_names=class_names,\n",
    "    digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Per-Class Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PER-CLASS PERFORMANCE (All classes must perform)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Compute per-class metrics\n",
    "per_class_recall = recall_score(test_true_classes, test_pred_classes, average=None, labels=range(len(class_names)))\n",
    "per_class_precision = precision_score(test_true_classes, test_pred_classes, average=None, labels=range(len(class_names)), zero_division=0)\n",
    "per_class_f1 = f1_score(test_true_classes, test_pred_classes, average=None, labels=range(len(class_names)), zero_division=0)\n",
    "per_class_support = [np.sum(test_true_classes == i) for i in range(len(class_names))]\n",
    "\n",
    "# Create performance DataFrame\n",
    "performance_df = pd.DataFrame({\n",
    "    'Class': class_names,\n",
    "    'Support': per_class_support,\n",
    "    'Recall': per_class_recall,\n",
    "    'Precision': per_class_precision,\n",
    "    'F1-Score': per_class_f1\n",
    "})\n",
    "\n",
    "# Sort by recall (ascending) to see poor performers first\n",
    "performance_df = performance_df.sort_values('Recall')\n",
    "\n",
    "print(\"\\n\" + performance_df.to_string(index=False))\n",
    "\n",
    "# Flag poor performers\n",
    "min_recall = CONFIG['evaluation']['min_recall_threshold']\n",
    "poor_performers = performance_df[performance_df['Recall'] < min_recall]\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "if len(poor_performers) > 0:\n",
    "    print(f\"WARNING: {len(poor_performers)} classes with recall < {min_recall:.0%}:\")\n",
    "    print(poor_performers[['Class', 'Recall', 'Support']].to_string(index=False))\n",
    "    print(\"\\nThese classes may need attention in future iterations.\")\n",
    "else:\n",
    "    print(f\"All classes have recall >= {min_recall:.0%}.\")\n",
    "    \n",
    "# Visualize per-class performance\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Sort by class name for consistent display\n",
    "performance_df_sorted = performance_df.sort_values('Class')\n",
    "x = np.arange(len(class_names))\n",
    "width = 0.25\n",
    "\n",
    "ax.bar(x - width, performance_df_sorted['Recall'], width, label='Recall', alpha=0.8)\n",
    "ax.bar(x, performance_df_sorted['Precision'], width, label='Precision', alpha=0.8)\n",
    "ax.bar(x + width, performance_df_sorted['F1-Score'], width, label='F1-Score', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Class', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Per-Class Performance Metrics', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(performance_df_sorted['Class'], rotation=45, ha='right')\n",
    "ax.legend(fontsize=11)\n",
    "ax.axhline(y=min_recall, color='r', linestyle='--', linewidth=2, label=f'{min_recall:.0%} Threshold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "ax.set_ylim([0, 1.05])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'per_class_performance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nVisualization saved to {RESULTS_DIR / 'per_class_performance.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Sample Predictions Visualization\n",
    "\n",
    "Display random test samples with predictions to illustrate model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display 9 random test samples with predictions\n",
    "np.random.seed(RANDOM_SEED)\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    # Select random test image\n",
    "    idx = np.random.randint(len(test_generator.filenames))\n",
    "    img_path = TEST_DIR / test_generator.filenames[idx]\n",
    "    \n",
    "    try:\n",
    "        img = Image.open(img_path)\n",
    "        ax.imshow(img)\n",
    "        \n",
    "        # Get true and predicted labels\n",
    "        true_label = class_names[test_true_classes[idx]]\n",
    "        pred_label = class_names[test_pred_classes[idx]]\n",
    "        confidence = test_predictions[idx, test_pred_classes[idx]]\n",
    "        \n",
    "        # Color code: green if correct, red if incorrect\n",
    "        color = 'green' if true_label == pred_label else 'red'\n",
    "        \n",
    "        # Display labels\n",
    "        ax.set_title(\n",
    "            f\"True: {true_label}\\\\nPred: {pred_label}\\\\nConf: {confidence:.2%}\",\n",
    "            color=color,\n",
    "            fontsize=10,\n",
    "            fontweight='bold'\n",
    "        )\n",
    "        ax.axis('off')\n",
    "    except Exception as e:\n",
    "        ax.text(0.5, 0.5, f'Error loading image:\\\\n{str(e)}', \n",
    "                ha='center', va='center', transform=ax.transAxes)\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.suptitle('Sample Test Predictions (Green = Correct, Red = Incorrect)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'sample_predictions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Sample predictions visualization saved to {RESULTS_DIR / 'sample_predictions.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Confusion Matrix Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"CONFUSION MATRIX ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(test_true_classes, test_pred_classes)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=class_names,\n",
    "    yticklabels=class_names,\n",
    "    cbar_kws={'label': 'Count'}\n",
    ")\n",
    "plt.title('Confusion Matrix - Test Set', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nConfusion matrix saved to {RESULTS_DIR / 'confusion_matrix.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. High-Risk Error Analysis\n",
    "\n",
    "Check for critical misclassifications (malignant to benign)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"HIGH-RISK ERROR ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get class indices\n",
    "urgent_classes = CONFIG['evaluation']['urgent_classes']\n",
    "benign_classes = CONFIG['evaluation']['benign_classes']\n",
    "\n",
    "urgent_indices = [class_names.index(c) for c in urgent_classes if c in class_names]\n",
    "benign_indices = [class_names.index(c) for c in benign_classes if c in class_names]\n",
    "\n",
    "# Count malignant-to-benign errors (CRITICAL)\n",
    "malignant_to_benign = 0\n",
    "for m_idx in urgent_indices:\n",
    "    for b_idx in benign_indices:\n",
    "        malignant_to_benign += cm[m_idx, b_idx]\n",
    "\n",
    "print(f\"\\nCRITICAL ERRORS:\")\n",
    "print(f\"  Malignant to benign: {malignant_to_benign}\")\n",
    "print(f\"  Status: {'ACCEPTABLE' if malignant_to_benign <= 2 else 'TOO HIGH'}\")\n",
    "\n",
    "if malignant_to_benign > 2:\n",
    "    print(\"\\n  ACTION REQUIRED: Review model training strategy.\")\n",
    "    print(\"     Consider increasing class weights for malignant classes\")\n",
    "\n",
    "# Check inflammatory dominance\n",
    "if 'inflammatory' in class_names:\n",
    "    inflammatory_idx = class_names.index('inflammatory')\n",
    "    total_pred_inflammatory = cm[:, inflammatory_idx].sum()\n",
    "    pct_inflammatory = total_pred_inflammatory / len(test_true_classes)\n",
    "    \n",
    "    print(f\"\\nINFLAMMATORY DOMINANCE CHECK:\")\n",
    "    print(f\"  Predicted as inflammatory: {pct_inflammatory:.1%}\")\n",
    "    print(f\"  Status: {'OK' if pct_inflammatory < 0.50 else 'MODEL BIASED'}\")\n",
    "    \n",
    "    if pct_inflammatory >= 0.50:\n",
    "        print(\"\\n  ACTION REQUIRED: Model is over-predicting inflammatory class.\")\n",
    "        print(\"     Class weights may need adjustment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Urgent Case Recall Evaluation\n",
    "\n",
    "**Proposal Target:** ≥80% recall on urgent cases (malignant conditions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"URGENT CASE RECALL (Proposal Target: ≥80%)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate recall for urgent cases\n",
    "if len(urgent_indices) > 0:\n",
    "    urgent_mask = np.isin(test_true_classes, urgent_indices)\n",
    "    \n",
    "    if urgent_mask.sum() > 0:\n",
    "        urgent_predictions = test_pred_classes[urgent_mask]\n",
    "        urgent_true = test_true_classes[urgent_mask]\n",
    "        \n",
    "        urgent_recall = recall_score(\n",
    "            urgent_true,\n",
    "            urgent_predictions,\n",
    "            labels=urgent_indices,\n",
    "            average='macro',\n",
    "            zero_division=0\n",
    "        )\n",
    "        \n",
    "        urgent_target = CONFIG['evaluation']['urgent_recall_target']\n",
    "        \n",
    "        print(f\"\\nUrgent Case Recall: {urgent_recall:.1%}\")\n",
    "        print(f\"Target: ≥{urgent_target:.0%}\")\n",
    "        print(f\"Status: {'PASS' if urgent_recall >= urgent_target else 'FAIL'}\")\n",
    "        \n",
    "        # Per urgent class\n",
    "        print(f\"\\nPer Urgent Class:\")\n",
    "        for class_name in urgent_classes:\n",
    "            if class_name in class_names:\n",
    "                idx = class_names.index(class_name)\n",
    "                class_mask = test_true_classes == idx\n",
    "                if class_mask.sum() > 0:\n",
    "                    class_recall = np.mean(test_pred_classes[class_mask] == test_true_classes[class_mask])\n",
    "                    support = class_mask.sum()\n",
    "                    print(f\"  {class_name:35s}: {class_recall:.1%} (n={support})\")\n",
    "    else:\n",
    "        print(\"\\nNo urgent cases in test set.\")\n",
    "        urgent_recall = None\n",
    "else:\n",
    "    print(\"\\nNo urgent classes defined.\")\n",
    "    urgent_recall = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. FST-Stratified Evaluation\n",
    "\n",
    "**Note:** This requires metadata with FST labels. If not available, this section will be skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"FST-STRATIFIED EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check if metadata with FST labels exists\n",
    "metadata_path = DATA_DIR / 'raw' / 'fitzpatrick17k' / 'fitzpatrick17k_fst_v_vi.csv'\n",
    "\n",
    "if metadata_path.exists():\n",
    "    print(\"\\nLoading metadata with FST labels...\")\n",
    "    \n",
    "    # Load metadata\n",
    "    metadata_df = pd.read_csv(metadata_path)\n",
    "    \n",
    "    # Create mapping from filename to FST\n",
    "    fst_mapping = {}\n",
    "    for _, row in metadata_df.iterrows():\n",
    "        filename = f\"{row['md5hash']}.jpg\"\n",
    "        fst_mapping[filename] = row['fitzpatrick_scale']\n",
    "    \n",
    "    # Get FST labels for test set\n",
    "    test_filenames = [Path(f).name for f in test_generator.filenames]\n",
    "    test_fst_labels = [fst_mapping.get(fn, None) for fn in test_filenames]\n",
    "    \n",
    "    # Check if we have FST labels\n",
    "    if None not in test_fst_labels:\n",
    "        test_fst_labels = np.array(test_fst_labels)\n",
    "        \n",
    "        # Evaluate separately for FST V and VI\n",
    "        fst_results = {}\n",
    "        \n",
    "        for fst in [5, 6]:\n",
    "            fst_mask = test_fst_labels == fst\n",
    "            \n",
    "            if fst_mask.sum() > 0:\n",
    "                fst_true = test_true_classes[fst_mask]\n",
    "                fst_pred = test_pred_classes[fst_mask]\n",
    "                \n",
    "                fst_results[fst] = {\n",
    "                    'n': fst_mask.sum(),\n",
    "                    'accuracy': np.mean(fst_pred == fst_true),\n",
    "                    'recall': recall_score(fst_true, fst_pred, average='macro', zero_division=0),\n",
    "                    'precision': precision_score(fst_true, fst_pred, average='macro', zero_division=0),\n",
    "                    'f1': f1_score(fst_true, fst_pred, average='macro', zero_division=0)\n",
    "                }\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"\\nFST V (n={fst_results[5]['n']}):\")\n",
    "        print(f\"  Accuracy:  {fst_results[5]['accuracy']:.1%}\")\n",
    "        print(f\"  Recall:    {fst_results[5]['recall']:.1%}\")\n",
    "        print(f\"  Precision: {fst_results[5]['precision']:.1%}\")\n",
    "        print(f\"  F1-Score:  {fst_results[5]['f1']:.1%}\")\n",
    "        \n",
    "        print(f\"\\nFST VI (n={fst_results[6]['n']}):\")\n",
    "        print(f\"  Accuracy:  {fst_results[6]['accuracy']:.1%}\")\n",
    "        print(f\"  Recall:    {fst_results[6]['recall']:.1%}\")\n",
    "        print(f\"  Precision: {fst_results[6]['precision']:.1%}\")\n",
    "        print(f\"  F1-Score:  {fst_results[6]['f1']:.1%}\")\n",
    "        \n",
    "        # Check equity gap\n",
    "        recall_gap = abs(fst_results[5]['recall'] - fst_results[6]['recall'])\n",
    "        equity_threshold = CONFIG['evaluation']['fst_equity_gap_threshold']\n",
    "        \n",
    "        print(f\"\\nFST Equity Gap: {recall_gap:.1%} (target <{equity_threshold:.0%})\")\n",
    "        print(f\"  Target: <{equity_threshold:.0%}\")\n",
    "        print(f\"  Status: {'PASS' if recall_gap < equity_threshold else 'FAIL'}\")\n",
    "        \n",
    "        if recall_gap >= equity_threshold:\n",
    "            print(\"\\n  WARNING: Performance gap exceeds threshold.\")\n",
    "            print(\"     Model shows FST bias - may need stratified training\")\n",
    "        \n",
    "        # Visualize FST comparison\n",
    "        fst_comparison = pd.DataFrame({\n",
    "            'FST V': [fst_results[5]['recall'], fst_results[5]['precision'], fst_results[5]['f1']],\n",
    "            'FST VI': [fst_results[6]['recall'], fst_results[6]['precision'], fst_results[6]['f1']]\n",
    "        }, index=['Recall', 'Precision', 'F1-Score'])\n",
    "        \n",
    "        ax = fst_comparison.plot(kind='bar', figsize=(10, 6), rot=0)\n",
    "        ax.set_title('FST-Stratified Performance Comparison', fontsize=14, fontweight='bold')\n",
    "        ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "        ax.set_xlabel('Metric', fontsize=12, fontweight='bold')\n",
    "        ax.legend(title='Fitzpatrick Skin Type', fontsize=11)\n",
    "        ax.axhline(y=0.80, color='r', linestyle='--', linewidth=2, label='80% Target')\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        ax.set_ylim([0, 1.05])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(RESULTS_DIR / 'fst_comparison.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\nFST comparison saved to {RESULTS_DIR / 'fst_comparison.png'}\")\n",
    "    else:\n",
    "        print(\"\\nFST labels not available for all test images.\")\n",
    "        fst_results = None\n",
    "        recall_gap = None\n",
    "else:\n",
    "    print(\"\\nMetadata file not found; skipping FST-stratified evaluation.\")\n",
    "    print(f\"   Expected: {metadata_path}\")\n",
    "    fst_results = None\n",
    "    recall_gap = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Success Criteria Checklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"CAPSTONE PROJECT SUCCESS CRITERIA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "criteria = {\n",
    "    '1. Overall accuracy >70%': test_accuracy > 0.70,\n",
    "    '2. All classes recall >50%': all(per_class_recall > 0.50),\n",
    "    '3. Malignant-to-benign errors ≤2': malignant_to_benign <= 2,\n",
    "}\n",
    "\n",
    "# Add urgent recall criterion if available\n",
    "if urgent_recall is not None:\n",
    "    criteria['4. Urgent case recall ≥80%'] = urgent_recall >= 0.80\n",
    "\n",
    "# Add FST equity criterion if available\n",
    "if recall_gap is not None:\n",
    "    criteria['5. FST equity gap <10%'] = recall_gap < 0.10\n",
    "\n",
    "# Add inflammatory dominance check\n",
    "if 'inflammatory' in class_names:\n",
    "    criteria['6. Model not biased to inflammatory'] = pct_inflammatory < 0.50\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "for criterion, passed in criteria.items():\n",
    "    status = \"PASS\" if passed else \"FAIL\"\n",
    "    print(f\"  {status} {criterion}\")\n",
    "\n",
    "all_passed = all(criteria.values())\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "if all_passed:\n",
    "    print(\"All criteria met; model ready for deployment.\")\n",
    "else:\n",
    "    failed_count = sum([not v for v in criteria.values()])\n",
    "    print(f\"{failed_count} criteria not met; review and retrain.\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19. Generate Final Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive JSON report\n",
    "final_report = {\n",
    "    'model_info': {\n",
    "        'architecture': CONFIG['model']['architecture'],\n",
    "        'input_shape': CONFIG['model']['input_shape'],\n",
    "        'num_classes': CONFIG['model']['num_classes'],\n",
    "        'total_params': int(model.count_params()),\n",
    "        'dropout_rate': CONFIG['model']['dropout_rate']\n",
    "    },\n",
    "    'training_config': {\n",
    "        'optimizer': 'Adam',\n",
    "        'initial_lr': CONFIG['training']['initial_lr'],\n",
    "        'finetune_lr': CONFIG['training']['finetune_lr'],\n",
    "        'batch_size': CONFIG['training']['batch_size'],\n",
    "        'phase1_epochs': CONFIG['training']['phase1_epochs'],\n",
    "        'phase2_epochs': CONFIG['training']['phase2_epochs'],\n",
    "        'total_epochs': len(history['loss']),\n",
    "        'class_weighting': 'enabled' if class_weight_dict else 'disabled',\n",
    "        'augmentation': 'applied during preprocessing',\n",
    "        'random_seed': RANDOM_SEED\n",
    "    },\n",
    "    'dataset': {\n",
    "        'train_samples': train_total,\n",
    "        'val_samples': val_total,\n",
    "        'test_samples': test_total,\n",
    "        'train_augmented': True,\n",
    "        'class_counts': train_counts\n",
    "    },\n",
    "    'performance': {\n",
    "        'overall': {\n",
    "            'test_accuracy': float(test_accuracy),\n",
    "            'test_recall': float(test_recall),\n",
    "            'test_precision': float(test_precision),\n",
    "            'test_f1': float(test_f1)\n",
    "        },\n",
    "        'per_class': {\n",
    "            class_names[i]: {\n",
    "                'recall': float(per_class_recall[i]),\n",
    "                'precision': float(per_class_precision[i]),\n",
    "                'f1': float(per_class_f1[i]),\n",
    "                'support': int(per_class_support[i])\n",
    "            }\n",
    "            for i in range(len(class_names))\n",
    "        },\n",
    "        'high_risk_errors': {\n",
    "            'malignant_to_benign': int(malignant_to_benign)\n",
    "        }\n",
    "    },\n",
    "    'proposal_objectives': {\n",
    "        'all_classes_viable': all(per_class_recall > 0.50)\n",
    "    },\n",
    "    'timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "# Add urgent case metrics if available\n",
    "if urgent_recall is not None:\n",
    "    final_report['performance']['urgent_cases'] = {\n",
    "        'recall': float(urgent_recall),\n",
    "        'target': CONFIG['evaluation']['urgent_recall_target'],\n",
    "        'status': 'PASS' if urgent_recall >= CONFIG['evaluation']['urgent_recall_target'] else 'FAIL'\n",
    "    }\n",
    "    final_report['proposal_objectives']['objective_2_recall_target'] = urgent_recall >= CONFIG['evaluation']['urgent_recall_target']\n",
    "\n",
    "# Add FST metrics if available\n",
    "if fst_results is not None:\n",
    "    final_report['performance']['fst_stratified'] = {\n",
    "        'fst_v': {\n",
    "            'n': int(fst_results[5]['n']),\n",
    "            'accuracy': float(fst_results[5]['accuracy']),\n",
    "            'recall': float(fst_results[5]['recall']),\n",
    "            'precision': float(fst_results[5]['precision']),\n",
    "            'f1': float(fst_results[5]['f1'])\n",
    "        },\n",
    "        'fst_vi': {\n",
    "            'n': int(fst_results[6]['n']),\n",
    "            'accuracy': float(fst_results[6]['accuracy']),\n",
    "            'recall': float(fst_results[6]['recall']),\n",
    "            'precision': float(fst_results[6]['precision']),\n",
    "            'f1': float(fst_results[6]['f1'])\n",
    "        },\n",
    "        'equity_gap': float(recall_gap),\n",
    "        'equity_status': 'PASS' if recall_gap < CONFIG['evaluation']['fst_equity_gap_threshold'] else 'FAIL'\n",
    "    }\n",
    "    final_report['proposal_objectives']['objective_4_fst_equity'] = recall_gap < CONFIG['evaluation']['fst_equity_gap_threshold']\n",
    "\n",
    "# Save report\n",
    "report_path = RESULTS_DIR / 'final_report.json'\n",
    "with open(report_path, 'w') as f:\n",
    "    json.dump(final_report, f, indent=2)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL REPORT SAVED\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Location: {report_path}\")\n",
    "print(\"\\nReport contents:\")\n",
    "print(json.dumps(final_report, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20. Save Model & Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"SAVING MODEL & ARTIFACTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save final model\n",
    "final_model_path = MODEL_DIR / 'dermoai_final_model.keras'\n",
    "model.save(final_model_path)\n",
    "print(f\"\\nModel saved: {final_model_path}\")\n",
    "\n",
    "# Model already saved via checkpoint: best_model.keras\n",
    "print(f\"Best model checkpoint: {MODEL_DIR / 'best_model.keras'}\")\n",
    "\n",
    "# Class names already saved\n",
    "print(f\"Class names: {MODEL_DIR / 'class_names.json'}\")\n",
    "\n",
    "# Save training history\n",
    "history_df = pd.DataFrame(history)\n",
    "history_csv_path = RESULTS_DIR / 'training_history.csv'\n",
    "history_df.to_csv(history_csv_path, index=False)\n",
    "print(f\"Training history: {history_csv_path}\")\n",
    "\n",
    "# Save configuration\n",
    "config_path = MODEL_DIR / 'training_config.json'\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(CONFIG, f, indent=2)\n",
    "print(f\"Training config: {config_path}\")\n",
    "\n",
    "# Create model card\n",
    "model_card = f\"\"\"# DermoAI Model Card\n",
    "\n",
    "## Model Information\n",
    "- **Architecture:** {CONFIG['model']['architecture']} (pretrained on ImageNet)\n",
    "- **Task:** Multi-class skin lesion classification ({CONFIG['model']['num_classes']} classes)\n",
    "- **Target Population:** Fitzpatrick Skin Types V-VI (African skin tones)\n",
    "- **Input:** {CONFIG['model']['input_shape'][0]}×{CONFIG['model']['input_shape'][1]} RGB images\n",
    "- **Output:** Softmax probabilities over {CONFIG['model']['num_classes']} classes\n",
    "- **Total Parameters:** {model.count_params():,}\n",
    "- **Training Date:** {datetime.now().strftime('%Y-%m-%d')}\n",
    "\n",
    "## Performance Metrics\n",
    "- **Overall Accuracy:** {test_accuracy:.1%}\n",
    "- **Macro Recall:** {test_recall:.1%}\n",
    "- **Macro Precision:** {test_precision:.1%}\n",
    "- **Macro F1-Score:** {test_f1:.1%}\n",
    "\"\"\"\n",
    "\n",
    "if urgent_recall is not None:\n",
    "    model_card += f\"- **Urgent Case Recall:** {urgent_recall:.1%} (Target: ≥80%)\\n\"\n",
    "\n",
    "if recall_gap is not None:\n",
    "    model_card += f\"- **FST Equity Gap:** {recall_gap:.1%} (Target: <10%)\\n\"\n",
    "\n",
    "model_card += f\"\"\"\n",
    "## Training Data\n",
    "- **Total Training Samples:** {train_total} (augmented)\n",
    "- **Validation Samples:** {val_total} (original)\n",
    "- **Test Samples:** {test_total} (original)\n",
    "- **Augmentation:** Flips, rotation (±15°), brightness/contrast (±0.1)\n",
    "- **Class Weighting:** {'Enabled' if class_weight_dict else 'Disabled'}\n",
    "- **Training Strategy:** Two-phase (head training + fine-tuning)\n",
    "\n",
    "## Classes\n",
    "{chr(10).join([f'{i+1}. {name}' for i, name in enumerate(class_names)])}\n",
    "\n",
    "## Limitations\n",
    "- Trained on dermatological images from Fitzpatrick17k dataset\n",
    "- Optimized for FST V-VI; performance on other skin types not validated\n",
    "- May not generalize to non-dermatoscopic images\n",
    "- Performance varies by class (see per-class metrics in final_report.json)\n",
    "- Requires clinical validation before deployment\n",
    "\n",
    "## Intended Use\n",
    "- **Primary:** Triage support for frontline health workers in Rwanda\n",
    "- **Not for:** Direct diagnosis or treatment decisions without specialist oversight\n",
    "- **Deployment Context:** Low-resource healthcare settings with limited specialist access\n",
    "\n",
    "## Ethical Considerations\n",
    "- Model trained exclusively on FST V-VI to address historical bias in dermatology AI\n",
    "- Class imbalance addressed via balanced class weighting\n",
    "- High-risk errors (malignant to benign) monitored: {malignant_to_benign} cases\n",
    "- FST equity gap monitored to prevent within-group disparities\n",
    "\n",
    "## Citation\n",
    "If you use this model, please cite:\n",
    "- DermoAI Capstone Project, 2026\n",
    "- Fitzpatrick17k Dataset: Groh et al., 2021\n",
    "\"\"\"\n",
    "\n",
    "model_card_path = MODEL_DIR / 'MODEL_CARD.md'\n",
    "with open(model_card_path, 'w') as f:\n",
    "    f.write(model_card)\n",
    "print(f\"Model card: {model_card_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ALL FILES SAVED SUCCESSFULLY\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nOutput files:\")\n",
    "print(f\"  Models:\")\n",
    "print(f\"    - {final_model_path}\")\n",
    "print(f\"    - {MODEL_DIR / 'best_model.keras'}\")\n",
    "print(f\"    - {MODEL_DIR / 'class_names.json'}\")\n",
    "print(f\"    - {config_path}\")\n",
    "print(f\"    - {model_card_path}\")\n",
    "print(f\"\\n  Results:\")\n",
    "print(f\"    - {report_path}\")\n",
    "print(f\"    - {history_csv_path}\")\n",
    "print(f\"    - {RESULTS_DIR / 'training_history.png'}\")\n",
    "print(f\"    - {RESULTS_DIR / 'per_class_performance.png'}\")\n",
    "print(f\"    - {RESULTS_DIR / 'confusion_matrix.png'}\")\n",
    "if fst_results is not None:\n",
    "    print(f\"    - {RESULTS_DIR / 'fst_comparison.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 21. Summary\n",
    "\n",
    "This notebook trains a MobileNetV2 CNN on augmented FST V-VI skin lesion data with class weighting, two-phase training (classifier head then fine-tuning), and evaluation including FST-stratified metrics, urgent-case recall, per-class performance, and high-risk error analysis. Outputs include the saved model, training history, and final report.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
